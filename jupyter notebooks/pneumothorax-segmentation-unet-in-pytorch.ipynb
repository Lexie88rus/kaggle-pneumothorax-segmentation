{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch UNet Model for Pneumothorax Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pydicom\n",
    "import random\n",
    "\n",
    "# import image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# import matplotlib for visualization\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "\n",
    "# Import rle utils\n",
    "import sys\n",
    "sys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n",
    "from mask_functions import rle2mask, mask2rle # import mask utilities\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.6/site-packages (0.3.0)\r\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.6/site-packages (from albumentations) (4.1.0.25)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from albumentations) (5.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from albumentations) (1.16.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from albumentations) (1.2.1)\r\n",
      "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from albumentations) (0.2.6)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.12.0)\r\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /opt/conda/lib/python3.6/site-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.15.0)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.2)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.0.3)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (6.0.0)\r\n",
      "Requirement already satisfied: imageio>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5.0)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.0.3)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (41.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n",
    "    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n",
    "    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n",
    "    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion\n",
    ")\n",
    "from albumentations.pytorch import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading utility\n",
    "def load_data(datafilepath = '../input/siim-train-test/siim/', healthy_num = 2000):\n",
    "    '''\n",
    "    Function to load the dataset.\n",
    "    INPUT:\n",
    "        datafilepath - path to directory containing the dataset.\n",
    "    OUTPUT:\n",
    "        train_fns - train dataset\n",
    "        train_fns - test dataset\n",
    "        df_masks - pandas dataframe containing masks for train dataset\n",
    "    '''\n",
    "    # Load full training and test sets\n",
    "    train_fns = sorted(glob(datafilepath + 'dicom-images-train/*/*/*.dcm'))\n",
    "    test_fns = sorted(glob(datafilepath + 'dicom-images-test/*/*/*.dcm'))\n",
    "    # Load csv masks\n",
    "    df_masks = pd.read_csv(datafilepath + 'train-rle.csv', index_col='ImageId')\n",
    "    # create a list of filenames with images to use\n",
    "    \n",
    "    counter = 0\n",
    "    files_list = []\n",
    "    for fname in train_fns:\n",
    "        try:\n",
    "            if '-1' in df_masks.loc[fname.split('/')[-1][:-4],' EncodedPixels']:\n",
    "                if counter <= healthy_num:\n",
    "                    files_list.append(fname)\n",
    "                    counter += 1\n",
    "            else:\n",
    "                files_list.append(fname)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return train_fns, test_fns, df_masks, files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    \"\"\"\n",
    "    Function performs the linear normalizarion of the array.\n",
    "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
    "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
    "    INPUT:\n",
    "        arr - orginal numpy array\n",
    "    OUTPUT:\n",
    "        arr - normalized numpy array\n",
    "    \"\"\"\n",
    "    arr = arr.astype('float')\n",
    "    # Do not touch the alpha channel\n",
    "    for i in range(3):\n",
    "        minval = arr[...,i].min()\n",
    "        maxval = arr[...,i].max()\n",
    "        if minval != maxval:\n",
    "            arr[...,i] -= minval\n",
    "            arr[...,i] *= (255.0/(maxval-minval))\n",
    "    return arr\n",
    "\n",
    "def normalize_image(img):\n",
    "    \"\"\"\n",
    "    Function performs the normalization of the image.\n",
    "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
    "    INPUT:\n",
    "        image - PIL image to be normalized\n",
    "    OUTPUT:\n",
    "        new_img - PIL image normalized\n",
    "    \"\"\"\n",
    "    arr = np.array(img)\n",
    "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'RGB')\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class PneumothoraxDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for pneumothorax segmentation.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, fns, df_masks, files_list, transform=True, size = (224, 224), mode = 'train'):\n",
    "        '''\n",
    "        INPUT:\n",
    "            fns - glob containing the images\n",
    "            df_masks - dataframe containing image masks\n",
    "            transform (optional) - enable transforms for the images\n",
    "        '''\n",
    "        self.labels_frame = df_masks\n",
    "        self.fns = fns\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "        self.transforms_mask = transforms.Compose([transforms.Resize(self.size), transforms.ToTensor()])\n",
    "        self.transforms_image = transforms.Compose([transforms.Resize(self.size), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        self.alb_transforms = Compose([HorizontalFlip(p=0.5),\n",
    "                                             OneOf([RandomContrast(),\n",
    "                                                    RandomGamma(),\n",
    "                                                    RandomBrightness(),], p=0.3),\n",
    "                                             OneOf([ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                                                    GridDistortion(),\n",
    "                                                    OpticalDistortion(distort_limit=2, shift_limit=0.5),], p=0.3)])\n",
    "        self.mode = mode\n",
    "        self.files_list = files_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if (self.mode == 'validation'):\n",
    "            return len(self.fns)\n",
    "        else:\n",
    "            return len(self.files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Function to get items from dataset by idx.\n",
    "        INPUT:\n",
    "            idx - id of the image in the dataset\n",
    "        '''\n",
    "        # image height and width\n",
    "        im_height = 1024\n",
    "        im_width = 1024\n",
    "        # image channels\n",
    "        im_chan = 1\n",
    "\n",
    "        # get train image and mask\n",
    "        np_image = np.zeros((im_height, im_width, im_chan), dtype=np.uint8)\n",
    "        np_mask = np.zeros((im_height, im_width, 1), dtype=np.bool)\n",
    "        \n",
    "        # if validation then return filename instead of mask\n",
    "        if self.mode == 'validation':\n",
    "            # read dcm file with image\n",
    "            dataset = pydicom.read_file(self.fns[idx])\n",
    "            np_image = np.expand_dims(dataset.pixel_array, axis=2)\n",
    "        \n",
    "            image = Image.fromarray(np_image.reshape(im_height, im_width) , 'L')\n",
    "            image = image.convert('RGB')\n",
    "            \n",
    "            image = self.transforms_image(image)\n",
    "            return [image, self.fns[idx].split('/')[-1][:-4]]\n",
    "        \n",
    "        # read dcm file with image\n",
    "        dataset = pydicom.read_file(self.files_list[idx])\n",
    "        np_image = np.expand_dims(dataset.pixel_array, axis=2)\n",
    "\n",
    "        # load mask\n",
    "        try:\n",
    "            # no pneumothorax\n",
    "            if '-1' in self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels']:\n",
    "                np_mask = np.zeros((im_height, im_width, 1), dtype=np.bool)\n",
    "            else:\n",
    "                # there is pneumothorax\n",
    "                if type(self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels']) == str:\n",
    "                    np_mask = np.expand_dims(rle2mask(self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels'], im_height, im_width), axis=2)\n",
    "                else:\n",
    "                    np_mask = np.zeros((1024, 1024, 1))\n",
    "                    for x in self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels']:\n",
    "                        np_mask =  np_mask + np.expand_dims(rle2mask(x, 1024, 1024), axis=2)\n",
    "        except KeyError:\n",
    "            # couldn't find mask in dataframe\n",
    "            np_mask = np.zeros((im_height, im_width, 1), dtype=np.bool) # Assume missing masks are empty masks.\n",
    "\n",
    "        # convert to PIL\n",
    "        image = Image.fromarray(np_image.reshape(im_height, im_width) , 'L')\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        np_mask = np.transpose(np_mask)\n",
    "        mask = Image.fromarray(np_mask.reshape(im_height, im_width).astype(np.uint8) , 'L')\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.alb_transforms(image=np.array(image), mask=np.array(mask))\n",
    "            image = Image.fromarray(augmented['image'], 'RGB')\n",
    "            \n",
    "            mask = Image.fromarray(augmented['mask'], 'L')\n",
    "        \n",
    "        # apply required transforms normalization, reshape and convert to tensor\n",
    "        #image = normalize_image(image)\n",
    "        image = self.transforms_image(image)\n",
    "        mask = self.transforms_mask(mask)\n",
    "        \n",
    "        # convert to tensor and clip mask\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        mask = np.clip(mask, 0, 1)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\n",
    "def get_iou_vector(A, B):    \n",
    "    batch_size = A.shape[0]\n",
    "    metric = 0.0\n",
    "        \n",
    "    for batch in range(batch_size):\n",
    "        p, t = A[batch].reshape(224,224), B[batch]\n",
    "        true = np.sum(t)\n",
    "        pred = np.sum(p)\n",
    "        \n",
    "        # deal with empty mask first\n",
    "        if true == 0:\n",
    "            metric += (pred == 0)\n",
    "            continue\n",
    "        \n",
    "        # non empty mask case.  Union is never empty \n",
    "        # hence it is safe to divide by its number of pixels\n",
    "        \n",
    "        intersection = np.sum(t * p)\n",
    "        union = true + pred - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # iou metric is a stepwise approximation of the real iou over 0.5\n",
    "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
    "              \n",
    "        metric += iou\n",
    "        \n",
    "    # teake the average over all images in batch\n",
    "    metric /= batch_size\n",
    "    return metric\n",
    "\n",
    "def my_iou_metric(pred, label):\n",
    "    return get_iou_vector((pred > 0.5).float().detach().cpu().numpy(), label.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_pred, y_true):\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred = y_pred.float()\n",
    "    y_pred_f = (y_pred.view(-1) >= 0.5).float()\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * torch.sum(intersection) / (torch.sum(y_true_f) + torch.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_pred, y_true):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * torch.sum(intersection) + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_pred, y_true):\n",
    "    criterion = nn.BCELoss()\n",
    "    return criterion(y_pred.reshape(-1,224,224), y_true) + dice_loss(y_pred.reshape(-1,224,224), y_true)\n",
    "\n",
    "def bce_logdice_loss(y_pred, y_true):\n",
    "    criterion = nn.BCELoss()\n",
    "    return criterion(y_pred.reshape(-1,224,224), y_true) - torch.log(1. - dice_loss(y_pred.reshape(-1,224,224), y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg11(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return self.final(dec1)\n",
    "\n",
    "\n",
    "def unet11(pretrained=False, **kwargs):\n",
    "    \"\"\"\n",
    "    pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "            carvana - all weights are pre-trained on\n",
    "                Kaggle: Carvana dataset https://www.kaggle.com/c/carvana-image-masking-challenge\n",
    "    \"\"\"\n",
    "    model = UNet11(pretrained=pretrained, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, size=None, scale_factor=None, mode='nearest', align_corners=False):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.scale_factor = scale_factor\n",
    "        self.align_corners = align_corners\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, size=self.size, scale_factor=self.scale_factor, \n",
    "                        mode=self.mode, align_corners=self.align_corners)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                Interpolate(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class AlbuNet(nn.Module):\n",
    "    \"\"\"\n",
    "        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n",
    "        Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with resnet34\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(128 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(64 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec0), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec0)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class UNet16(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network used\n",
    "            True - encoder pre-trained with VGG16\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg16(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[5],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[7],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(self.encoder[10],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[12],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[14],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv4 = nn.Sequential(self.encoder[17],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[19],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[21],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv5 = nn.Sequential(self.encoder[24],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[26],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[28],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(self.pool(conv1))\n",
    "        conv3 = self.conv3(self.pool(conv2))\n",
    "        conv4 = self.conv4(self.pool(conv3))\n",
    "        conv5 = self.conv5(self.pool(conv4))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "\n",
    "        if self.num_classes > 1:\n",
    "            x_out = F.log_softmax(self.final(dec1), dim=1)\n",
    "        else:\n",
    "            x_out = self.final(dec1)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, trainloader, testloader, optimizer, epochs):\n",
    "    model.to(device)\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    running_iou = 0\n",
    "    print_every = 100\n",
    "    \n",
    "    # learning rate cosine annealing\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(trainloader), eta_min=0.0000001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        scheduler.step()\n",
    "        \n",
    "        for inputs, labels in trainloader:\n",
    "\n",
    "            steps += 1\n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = bce_dice_loss(torch.sigmoid(outputs), labels.reshape(-1, 224, 224).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss\n",
    "            running_iou += my_iou_metric(torch.sigmoid(outputs), labels.reshape(-1, 224, 224).float())\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                iou = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in testloader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model.forward(inputs)\n",
    "                        \n",
    "                        test_loss += bce_dice_loss(torch.sigmoid(outputs), labels.reshape(-1, 224, 224).float())\n",
    "                        \n",
    "                        iou += my_iou_metric(torch.sigmoid(outputs), labels.reshape(-1, 224, 224).float())\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                      f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                      f\"Train IOU: {running_iou/print_every:.3f}.. \"\n",
    "                      f\"Test IOU: {iou/len(testloader):.3f}.. \")\n",
    " \n",
    "                running_loss = 0\n",
    "                running_iou = 0\n",
    "                   \n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: \n",
      "\n",
      "Preparing the dataset: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /tmp/.cache/torch/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 553433881/553433881 [00:21<00:00, 25993590.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model: \n",
      "\n",
      "Epoch 1/80.. Train loss: 1.326.. Test loss: 1.043.. Train IOU: 0.196.. Test IOU: 0.499.. \n",
      "Epoch 1/80.. Train loss: 1.010.. Test loss: 0.965.. Train IOU: 0.474.. Test IOU: 0.495.. \n",
      "Epoch 2/80.. Train loss: 0.932.. Test loss: 0.910.. Train IOU: 0.392.. Test IOU: 0.412.. \n",
      "Epoch 2/80.. Train loss: 0.880.. Test loss: 0.830.. Train IOU: 0.257.. Test IOU: 0.324.. \n",
      "Epoch 3/80.. Train loss: 0.838.. Test loss: 0.802.. Train IOU: 0.279.. Test IOU: 0.228.. \n",
      "Epoch 3/80.. Train loss: 0.762.. Test loss: 0.786.. Train IOU: 0.316.. Test IOU: 0.201.. \n",
      "Epoch 3/80.. Train loss: 0.791.. Test loss: 0.843.. Train IOU: 0.308.. Test IOU: 0.135.. \n",
      "Epoch 4/80.. Train loss: 0.737.. Test loss: 0.755.. Train IOU: 0.329.. Test IOU: 0.400.. \n",
      "Epoch 4/80.. Train loss: 0.711.. Test loss: 0.759.. Train IOU: 0.340.. Test IOU: 0.412.. \n",
      "Epoch 5/80.. Train loss: 0.702.. Test loss: 0.708.. Train IOU: 0.334.. Test IOU: 0.388.. \n",
      "Epoch 5/80.. Train loss: 0.696.. Test loss: 0.769.. Train IOU: 0.348.. Test IOU: 0.442.. \n",
      "Epoch 5/80.. Train loss: 0.689.. Test loss: 0.717.. Train IOU: 0.384.. Test IOU: 0.355.. \n",
      "Epoch 6/80.. Train loss: 0.648.. Test loss: 0.740.. Train IOU: 0.372.. Test IOU: 0.390.. \n",
      "Epoch 6/80.. Train loss: 0.655.. Test loss: 0.725.. Train IOU: 0.368.. Test IOU: 0.445.. \n",
      "Epoch 7/80.. Train loss: 0.669.. Test loss: 0.724.. Train IOU: 0.352.. Test IOU: 0.411.. \n",
      "Epoch 7/80.. Train loss: 0.641.. Test loss: 0.675.. Train IOU: 0.373.. Test IOU: 0.431.. \n",
      "Epoch 7/80.. Train loss: 0.616.. Test loss: 0.695.. Train IOU: 0.384.. Test IOU: 0.368.. \n",
      "Epoch 8/80.. Train loss: 0.631.. Test loss: 0.688.. Train IOU: 0.369.. Test IOU: 0.414.. \n",
      "Epoch 8/80.. Train loss: 0.605.. Test loss: 0.692.. Train IOU: 0.367.. Test IOU: 0.445.. \n",
      "Epoch 9/80.. Train loss: 0.626.. Test loss: 0.669.. Train IOU: 0.386.. Test IOU: 0.373.. \n",
      "Epoch 9/80.. Train loss: 0.584.. Test loss: 0.670.. Train IOU: 0.384.. Test IOU: 0.402.. \n",
      "Epoch 9/80.. Train loss: 0.615.. Test loss: 0.683.. Train IOU: 0.379.. Test IOU: 0.429.. \n",
      "Epoch 10/80.. Train loss: 0.613.. Test loss: 0.688.. Train IOU: 0.358.. Test IOU: 0.432.. \n",
      "Epoch 10/80.. Train loss: 0.589.. Test loss: 0.681.. Train IOU: 0.383.. Test IOU: 0.430.. \n",
      "Epoch 11/80.. Train loss: 0.555.. Test loss: 0.712.. Train IOU: 0.406.. Test IOU: 0.414.. \n",
      "Epoch 11/80.. Train loss: 0.592.. Test loss: 0.692.. Train IOU: 0.375.. Test IOU: 0.456.. \n",
      "Epoch 11/80.. Train loss: 0.575.. Test loss: 0.649.. Train IOU: 0.419.. Test IOU: 0.434.. \n",
      "Epoch 12/80.. Train loss: 0.570.. Test loss: 0.655.. Train IOU: 0.420.. Test IOU: 0.414.. \n",
      "Epoch 12/80.. Train loss: 0.561.. Test loss: 0.665.. Train IOU: 0.404.. Test IOU: 0.444.. \n",
      "Epoch 13/80.. Train loss: 0.547.. Test loss: 0.633.. Train IOU: 0.406.. Test IOU: 0.410.. \n",
      "Epoch 13/80.. Train loss: 0.539.. Test loss: 0.672.. Train IOU: 0.402.. Test IOU: 0.481.. \n",
      "Epoch 13/80.. Train loss: 0.547.. Test loss: 0.724.. Train IOU: 0.405.. Test IOU: 0.472.. \n",
      "Epoch 14/80.. Train loss: 0.542.. Test loss: 0.676.. Train IOU: 0.423.. Test IOU: 0.457.. \n",
      "Epoch 14/80.. Train loss: 0.532.. Test loss: 0.651.. Train IOU: 0.425.. Test IOU: 0.366.. \n",
      "Epoch 15/80.. Train loss: 0.553.. Test loss: 0.669.. Train IOU: 0.434.. Test IOU: 0.465.. \n",
      "Epoch 15/80.. Train loss: 0.515.. Test loss: 0.625.. Train IOU: 0.420.. Test IOU: 0.405.. \n",
      "Epoch 15/80.. Train loss: 0.560.. Test loss: 0.652.. Train IOU: 0.399.. Test IOU: 0.378.. \n",
      "Epoch 16/80.. Train loss: 0.522.. Test loss: 0.671.. Train IOU: 0.443.. Test IOU: 0.475.. \n",
      "Epoch 16/80.. Train loss: 0.538.. Test loss: 0.629.. Train IOU: 0.438.. Test IOU: 0.357.. \n",
      "Epoch 17/80.. Train loss: 0.533.. Test loss: 0.624.. Train IOU: 0.408.. Test IOU: 0.367.. \n",
      "Epoch 17/80.. Train loss: 0.516.. Test loss: 0.646.. Train IOU: 0.421.. Test IOU: 0.356.. \n",
      "Epoch 18/80.. Train loss: 0.522.. Test loss: 0.621.. Train IOU: 0.441.. Test IOU: 0.329.. \n",
      "Epoch 18/80.. Train loss: 0.498.. Test loss: 0.642.. Train IOU: 0.430.. Test IOU: 0.446.. \n",
      "Epoch 18/80.. Train loss: 0.485.. Test loss: 0.721.. Train IOU: 0.466.. Test IOU: 0.483.. \n",
      "Epoch 19/80.. Train loss: 0.518.. Test loss: 0.648.. Train IOU: 0.427.. Test IOU: 0.338.. \n",
      "Epoch 19/80.. Train loss: 0.502.. Test loss: 0.630.. Train IOU: 0.456.. Test IOU: 0.408.. \n",
      "Epoch 20/80.. Train loss: 0.494.. Test loss: 0.644.. Train IOU: 0.441.. Test IOU: 0.429.. \n",
      "Epoch 20/80.. Train loss: 0.478.. Test loss: 0.650.. Train IOU: 0.427.. Test IOU: 0.416.. \n",
      "Epoch 20/80.. Train loss: 0.470.. Test loss: 0.618.. Train IOU: 0.450.. Test IOU: 0.450.. \n",
      "Epoch 21/80.. Train loss: 0.504.. Test loss: 0.650.. Train IOU: 0.447.. Test IOU: 0.459.. \n",
      "Epoch 21/80.. Train loss: 0.480.. Test loss: 0.638.. Train IOU: 0.454.. Test IOU: 0.466.. \n",
      "Epoch 22/80.. Train loss: 0.490.. Test loss: 0.773.. Train IOU: 0.451.. Test IOU: 0.378.. \n",
      "Epoch 22/80.. Train loss: 0.504.. Test loss: 0.633.. Train IOU: 0.430.. Test IOU: 0.442.. \n",
      "Epoch 22/80.. Train loss: 0.471.. Test loss: 0.642.. Train IOU: 0.452.. Test IOU: 0.479.. \n",
      "Epoch 23/80.. Train loss: 0.505.. Test loss: 0.674.. Train IOU: 0.463.. Test IOU: 0.430.. \n",
      "Epoch 23/80.. Train loss: 0.488.. Test loss: 0.632.. Train IOU: 0.436.. Test IOU: 0.380.. \n",
      "Epoch 24/80.. Train loss: 0.493.. Test loss: 0.620.. Train IOU: 0.451.. Test IOU: 0.431.. \n",
      "Epoch 24/80.. Train loss: 0.453.. Test loss: 0.684.. Train IOU: 0.462.. Test IOU: 0.492.. \n",
      "Epoch 24/80.. Train loss: 0.469.. Test loss: 0.653.. Train IOU: 0.459.. Test IOU: 0.415.. \n",
      "Epoch 25/80.. Train loss: 0.475.. Test loss: 0.598.. Train IOU: 0.451.. Test IOU: 0.424.. \n",
      "Epoch 25/80.. Train loss: 0.453.. Test loss: 0.653.. Train IOU: 0.470.. Test IOU: 0.470.. \n",
      "Epoch 26/80.. Train loss: 0.438.. Test loss: 0.640.. Train IOU: 0.466.. Test IOU: 0.378.. \n",
      "Epoch 26/80.. Train loss: 0.473.. Test loss: 0.595.. Train IOU: 0.453.. Test IOU: 0.410.. \n",
      "Epoch 26/80.. Train loss: 0.461.. Test loss: 0.648.. Train IOU: 0.474.. Test IOU: 0.397.. \n",
      "Epoch 27/80.. Train loss: 0.460.. Test loss: 0.667.. Train IOU: 0.459.. Test IOU: 0.457.. \n",
      "Epoch 27/80.. Train loss: 0.479.. Test loss: 0.641.. Train IOU: 0.477.. Test IOU: 0.467.. \n",
      "Epoch 28/80.. Train loss: 0.457.. Test loss: 0.619.. Train IOU: 0.478.. Test IOU: 0.477.. \n",
      "Epoch 28/80.. Train loss: 0.445.. Test loss: 0.683.. Train IOU: 0.458.. Test IOU: 0.490.. \n",
      "Epoch 28/80.. Train loss: 0.468.. Test loss: 0.671.. Train IOU: 0.469.. Test IOU: 0.272.. \n",
      "Epoch 29/80.. Train loss: 0.463.. Test loss: 0.612.. Train IOU: 0.451.. Test IOU: 0.445.. \n",
      "Epoch 29/80.. Train loss: 0.468.. Test loss: 0.636.. Train IOU: 0.486.. Test IOU: 0.423.. \n",
      "Epoch 30/80.. Train loss: 0.455.. Test loss: 0.670.. Train IOU: 0.461.. Test IOU: 0.458.. \n",
      "Epoch 30/80.. Train loss: 0.449.. Test loss: 0.660.. Train IOU: 0.479.. Test IOU: 0.474.. \n",
      "Epoch 30/80.. Train loss: 0.462.. Test loss: 0.638.. Train IOU: 0.465.. Test IOU: 0.426.. \n",
      "Epoch 31/80.. Train loss: 0.422.. Test loss: 0.676.. Train IOU: 0.479.. Test IOU: 0.489.. \n",
      "Epoch 31/80.. Train loss: 0.443.. Test loss: 0.632.. Train IOU: 0.490.. Test IOU: 0.438.. \n",
      "Epoch 32/80.. Train loss: 0.424.. Test loss: 0.716.. Train IOU: 0.477.. Test IOU: 0.484.. \n",
      "Epoch 32/80.. Train loss: 0.436.. Test loss: 0.616.. Train IOU: 0.484.. Test IOU: 0.410.. \n",
      "Epoch 32/80.. Train loss: 0.440.. Test loss: 0.629.. Train IOU: 0.469.. Test IOU: 0.429.. \n",
      "Epoch 33/80.. Train loss: 0.425.. Test loss: 0.640.. Train IOU: 0.490.. Test IOU: 0.453.. \n",
      "Epoch 33/80.. Train loss: 0.444.. Test loss: 0.690.. Train IOU: 0.471.. Test IOU: 0.480.. \n",
      "Epoch 34/80.. Train loss: 0.393.. Test loss: 0.685.. Train IOU: 0.484.. Test IOU: 0.439.. \n",
      "Epoch 34/80.. Train loss: 0.426.. Test loss: 0.615.. Train IOU: 0.488.. Test IOU: 0.477.. \n",
      "Epoch 35/80.. Train loss: 0.448.. Test loss: 0.627.. Train IOU: 0.465.. Test IOU: 0.471.. \n",
      "Epoch 35/80.. Train loss: 0.406.. Test loss: 0.648.. Train IOU: 0.499.. Test IOU: 0.464.. \n",
      "Epoch 35/80.. Train loss: 0.440.. Test loss: 0.688.. Train IOU: 0.511.. Test IOU: 0.468.. \n",
      "Epoch 36/80.. Train loss: 0.416.. Test loss: 0.625.. Train IOU: 0.479.. Test IOU: 0.460.. \n",
      "Epoch 36/80.. Train loss: 0.394.. Test loss: 0.627.. Train IOU: 0.516.. Test IOU: 0.465.. \n",
      "Epoch 37/80.. Train loss: 0.435.. Test loss: 0.650.. Train IOU: 0.489.. Test IOU: 0.451.. \n",
      "Epoch 37/80.. Train loss: 0.411.. Test loss: 0.643.. Train IOU: 0.475.. Test IOU: 0.459.. \n",
      "Epoch 37/80.. Train loss: 0.425.. Test loss: 0.623.. Train IOU: 0.482.. Test IOU: 0.443.. \n",
      "Epoch 38/80.. Train loss: 0.437.. Test loss: 0.615.. Train IOU: 0.495.. Test IOU: 0.431.. \n",
      "Epoch 38/80.. Train loss: 0.408.. Test loss: 0.700.. Train IOU: 0.488.. Test IOU: 0.481.. \n",
      "Epoch 39/80.. Train loss: 0.421.. Test loss: 0.653.. Train IOU: 0.500.. Test IOU: 0.474.. \n",
      "Epoch 39/80.. Train loss: 0.413.. Test loss: 0.639.. Train IOU: 0.497.. Test IOU: 0.443.. \n",
      "Epoch 39/80.. Train loss: 0.393.. Test loss: 0.616.. Train IOU: 0.506.. Test IOU: 0.475.. \n",
      "Epoch 40/80.. Train loss: 0.426.. Test loss: 0.606.. Train IOU: 0.494.. Test IOU: 0.424.. \n",
      "Epoch 40/80.. Train loss: 0.402.. Test loss: 0.621.. Train IOU: 0.491.. Test IOU: 0.402.. \n",
      "Epoch 41/80.. Train loss: 0.408.. Test loss: 0.645.. Train IOU: 0.513.. Test IOU: 0.414.. \n",
      "Epoch 41/80.. Train loss: 0.422.. Test loss: 0.639.. Train IOU: 0.495.. Test IOU: 0.409.. \n",
      "Epoch 41/80.. Train loss: 0.404.. Test loss: 0.654.. Train IOU: 0.483.. Test IOU: 0.455.. \n",
      "Epoch 42/80.. Train loss: 0.420.. Test loss: 0.652.. Train IOU: 0.486.. Test IOU: 0.445.. \n",
      "Epoch 42/80.. Train loss: 0.402.. Test loss: 0.675.. Train IOU: 0.528.. Test IOU: 0.448.. \n",
      "Epoch 43/80.. Train loss: 0.407.. Test loss: 0.703.. Train IOU: 0.499.. Test IOU: 0.483.. \n",
      "Epoch 43/80.. Train loss: 0.383.. Test loss: 0.664.. Train IOU: 0.525.. Test IOU: 0.482.. \n",
      "Epoch 43/80.. Train loss: 0.398.. Test loss: 0.612.. Train IOU: 0.496.. Test IOU: 0.456.. \n",
      "Epoch 44/80.. Train loss: 0.410.. Test loss: 0.647.. Train IOU: 0.498.. Test IOU: 0.438.. \n",
      "Epoch 44/80.. Train loss: 0.400.. Test loss: 0.630.. Train IOU: 0.517.. Test IOU: 0.461.. \n",
      "Epoch 45/80.. Train loss: 0.401.. Test loss: 0.686.. Train IOU: 0.492.. Test IOU: 0.477.. \n",
      "Epoch 45/80.. Train loss: 0.420.. Test loss: 0.656.. Train IOU: 0.505.. Test IOU: 0.488.. \n",
      "Epoch 45/80.. Train loss: 0.403.. Test loss: 0.693.. Train IOU: 0.512.. Test IOU: 0.482.. \n",
      "Epoch 46/80.. Train loss: 0.401.. Test loss: 0.628.. Train IOU: 0.532.. Test IOU: 0.450.. \n",
      "Epoch 46/80.. Train loss: 0.408.. Test loss: 0.657.. Train IOU: 0.479.. Test IOU: 0.468.. \n",
      "Epoch 47/80.. Train loss: 0.394.. Test loss: 0.598.. Train IOU: 0.513.. Test IOU: 0.450.. \n",
      "Epoch 47/80.. Train loss: 0.404.. Test loss: 0.647.. Train IOU: 0.506.. Test IOU: 0.463.. \n",
      "Epoch 47/80.. Train loss: 0.372.. Test loss: 0.631.. Train IOU: 0.517.. Test IOU: 0.465.. \n",
      "Epoch 48/80.. Train loss: 0.425.. Test loss: 0.651.. Train IOU: 0.529.. Test IOU: 0.470.. \n",
      "Epoch 48/80.. Train loss: 0.412.. Test loss: 0.611.. Train IOU: 0.484.. Test IOU: 0.420.. \n",
      "Epoch 49/80.. Train loss: 0.370.. Test loss: 0.626.. Train IOU: 0.494.. Test IOU: 0.441.. \n",
      "Epoch 49/80.. Train loss: 0.384.. Test loss: 0.683.. Train IOU: 0.510.. Test IOU: 0.458.. \n",
      "Epoch 49/80.. Train loss: 0.374.. Test loss: 0.662.. Train IOU: 0.516.. Test IOU: 0.481.. \n",
      "Epoch 50/80.. Train loss: 0.376.. Test loss: 0.602.. Train IOU: 0.512.. Test IOU: 0.439.. \n",
      "Epoch 50/80.. Train loss: 0.397.. Test loss: 0.625.. Train IOU: 0.506.. Test IOU: 0.442.. \n",
      "Epoch 51/80.. Train loss: 0.380.. Test loss: 0.630.. Train IOU: 0.511.. Test IOU: 0.450.. \n",
      "Epoch 51/80.. Train loss: 0.377.. Test loss: 0.649.. Train IOU: 0.530.. Test IOU: 0.441.. \n",
      "Epoch 52/80.. Train loss: 0.374.. Test loss: 0.635.. Train IOU: 0.497.. Test IOU: 0.479.. \n",
      "Epoch 52/80.. Train loss: 0.389.. Test loss: 0.604.. Train IOU: 0.516.. Test IOU: 0.460.. \n",
      "Epoch 52/80.. Train loss: 0.373.. Test loss: 0.597.. Train IOU: 0.511.. Test IOU: 0.440.. \n",
      "Epoch 53/80.. Train loss: 0.373.. Test loss: 0.619.. Train IOU: 0.503.. Test IOU: 0.469.. \n",
      "Epoch 53/80.. Train loss: 0.385.. Test loss: 0.623.. Train IOU: 0.527.. Test IOU: 0.464.. \n",
      "Epoch 54/80.. Train loss: 0.374.. Test loss: 0.627.. Train IOU: 0.533.. Test IOU: 0.447.. \n",
      "Epoch 54/80.. Train loss: 0.367.. Test loss: 0.661.. Train IOU: 0.522.. Test IOU: 0.463.. \n",
      "Epoch 54/80.. Train loss: 0.364.. Test loss: 0.634.. Train IOU: 0.537.. Test IOU: 0.456.. \n",
      "Epoch 55/80.. Train loss: 0.396.. Test loss: 0.681.. Train IOU: 0.497.. Test IOU: 0.452.. \n",
      "Epoch 55/80.. Train loss: 0.375.. Test loss: 0.630.. Train IOU: 0.520.. Test IOU: 0.462.. \n",
      "Epoch 56/80.. Train loss: 0.361.. Test loss: 0.626.. Train IOU: 0.529.. Test IOU: 0.449.. \n",
      "Epoch 56/80.. Train loss: 0.358.. Test loss: 0.614.. Train IOU: 0.524.. Test IOU: 0.444.. \n",
      "Epoch 56/80.. Train loss: 0.368.. Test loss: 0.657.. Train IOU: 0.526.. Test IOU: 0.471.. \n",
      "Epoch 57/80.. Train loss: 0.367.. Test loss: 0.603.. Train IOU: 0.533.. Test IOU: 0.471.. \n",
      "Epoch 57/80.. Train loss: 0.378.. Test loss: 0.612.. Train IOU: 0.518.. Test IOU: 0.453.. \n",
      "Epoch 58/80.. Train loss: 0.379.. Test loss: 0.657.. Train IOU: 0.523.. Test IOU: 0.455.. \n",
      "Epoch 58/80.. Train loss: 0.364.. Test loss: 0.632.. Train IOU: 0.526.. Test IOU: 0.426.. \n",
      "Epoch 58/80.. Train loss: 0.372.. Test loss: 0.669.. Train IOU: 0.511.. Test IOU: 0.454.. \n",
      "Epoch 59/80.. Train loss: 0.362.. Test loss: 0.656.. Train IOU: 0.518.. Test IOU: 0.461.. \n",
      "Epoch 59/80.. Train loss: 0.394.. Test loss: 0.639.. Train IOU: 0.517.. Test IOU: 0.443.. \n",
      "Epoch 60/80.. Train loss: 0.351.. Test loss: 0.651.. Train IOU: 0.540.. Test IOU: 0.452.. \n",
      "Epoch 60/80.. Train loss: 0.376.. Test loss: 0.637.. Train IOU: 0.545.. Test IOU: 0.446.. \n",
      "Epoch 60/80.. Train loss: 0.337.. Test loss: 0.666.. Train IOU: 0.525.. Test IOU: 0.464.. \n",
      "Epoch 61/80.. Train loss: 0.361.. Test loss: 0.638.. Train IOU: 0.529.. Test IOU: 0.487.. \n",
      "Epoch 61/80.. Train loss: 0.339.. Test loss: 0.685.. Train IOU: 0.545.. Test IOU: 0.489.. \n",
      "Epoch 62/80.. Train loss: 0.374.. Test loss: 0.691.. Train IOU: 0.525.. Test IOU: 0.457.. \n",
      "Epoch 62/80.. Train loss: 0.343.. Test loss: 0.656.. Train IOU: 0.551.. Test IOU: 0.463.. \n",
      "Epoch 62/80.. Train loss: 0.368.. Test loss: 0.692.. Train IOU: 0.530.. Test IOU: 0.487.. \n",
      "Epoch 63/80.. Train loss: 0.379.. Test loss: 0.655.. Train IOU: 0.536.. Test IOU: 0.456.. \n",
      "Epoch 63/80.. Train loss: 0.359.. Test loss: 0.661.. Train IOU: 0.528.. Test IOU: 0.484.. \n",
      "Epoch 64/80.. Train loss: 0.353.. Test loss: 0.666.. Train IOU: 0.531.. Test IOU: 0.465.. \n",
      "Epoch 64/80.. Train loss: 0.342.. Test loss: 0.644.. Train IOU: 0.530.. Test IOU: 0.507.. \n",
      "Epoch 64/80.. Train loss: 0.357.. Test loss: 0.647.. Train IOU: 0.547.. Test IOU: 0.490.. \n",
      "Epoch 65/80.. Train loss: 0.351.. Test loss: 0.656.. Train IOU: 0.524.. Test IOU: 0.455.. \n",
      "Epoch 65/80.. Train loss: 0.355.. Test loss: 0.617.. Train IOU: 0.530.. Test IOU: 0.429.. \n",
      "Epoch 66/80.. Train loss: 0.334.. Test loss: 0.646.. Train IOU: 0.544.. Test IOU: 0.453.. \n",
      "Epoch 66/80.. Train loss: 0.354.. Test loss: 0.620.. Train IOU: 0.520.. Test IOU: 0.444.. \n",
      "Epoch 66/80.. Train loss: 0.356.. Test loss: 0.657.. Train IOU: 0.544.. Test IOU: 0.448.. \n",
      "Epoch 67/80.. Train loss: 0.345.. Test loss: 0.603.. Train IOU: 0.527.. Test IOU: 0.463.. \n",
      "Epoch 67/80.. Train loss: 0.346.. Test loss: 0.608.. Train IOU: 0.535.. Test IOU: 0.443.. \n",
      "Epoch 68/80.. Train loss: 0.399.. Test loss: 0.633.. Train IOU: 0.519.. Test IOU: 0.467.. \n",
      "Epoch 68/80.. Train loss: 0.353.. Test loss: 0.635.. Train IOU: 0.533.. Test IOU: 0.450.. \n",
      "Epoch 69/80.. Train loss: 0.314.. Test loss: 0.696.. Train IOU: 0.555.. Test IOU: 0.475.. \n",
      "Epoch 69/80.. Train loss: 0.355.. Test loss: 0.591.. Train IOU: 0.547.. Test IOU: 0.448.. \n",
      "Epoch 69/80.. Train loss: 0.345.. Test loss: 0.636.. Train IOU: 0.537.. Test IOU: 0.478.. \n",
      "Epoch 70/80.. Train loss: 0.352.. Test loss: 0.616.. Train IOU: 0.531.. Test IOU: 0.478.. \n",
      "Epoch 70/80.. Train loss: 0.364.. Test loss: 0.621.. Train IOU: 0.528.. Test IOU: 0.480.. \n",
      "Epoch 71/80.. Train loss: 0.348.. Test loss: 0.659.. Train IOU: 0.543.. Test IOU: 0.481.. \n",
      "Epoch 71/80.. Train loss: 0.359.. Test loss: 0.630.. Train IOU: 0.528.. Test IOU: 0.467.. \n",
      "Epoch 71/80.. Train loss: 0.340.. Test loss: 0.605.. Train IOU: 0.536.. Test IOU: 0.403.. \n",
      "Epoch 72/80.. Train loss: 0.349.. Test loss: 0.647.. Train IOU: 0.546.. Test IOU: 0.445.. \n",
      "Epoch 72/80.. Train loss: 0.357.. Test loss: 0.614.. Train IOU: 0.536.. Test IOU: 0.442.. \n",
      "Epoch 73/80.. Train loss: 0.319.. Test loss: 0.627.. Train IOU: 0.526.. Test IOU: 0.467.. \n",
      "Epoch 73/80.. Train loss: 0.326.. Test loss: 0.649.. Train IOU: 0.543.. Test IOU: 0.418.. \n",
      "Epoch 73/80.. Train loss: 0.341.. Test loss: 0.598.. Train IOU: 0.545.. Test IOU: 0.458.. \n",
      "Epoch 74/80.. Train loss: 0.359.. Test loss: 0.651.. Train IOU: 0.535.. Test IOU: 0.480.. \n",
      "Epoch 74/80.. Train loss: 0.376.. Test loss: 0.663.. Train IOU: 0.545.. Test IOU: 0.452.. \n",
      "Epoch 75/80.. Train loss: 0.309.. Test loss: 0.562.. Train IOU: 0.554.. Test IOU: 0.448.. \n",
      "Epoch 75/80.. Train loss: 0.314.. Test loss: 0.655.. Train IOU: 0.558.. Test IOU: 0.436.. \n",
      "Epoch 75/80.. Train loss: 0.375.. Test loss: 0.627.. Train IOU: 0.541.. Test IOU: 0.442.. \n",
      "Epoch 76/80.. Train loss: 0.343.. Test loss: 0.619.. Train IOU: 0.554.. Test IOU: 0.441.. \n",
      "Epoch 76/80.. Train loss: 0.341.. Test loss: 0.635.. Train IOU: 0.517.. Test IOU: 0.477.. \n",
      "Epoch 77/80.. Train loss: 0.371.. Test loss: 0.634.. Train IOU: 0.533.. Test IOU: 0.463.. \n",
      "Epoch 77/80.. Train loss: 0.333.. Test loss: 0.615.. Train IOU: 0.546.. Test IOU: 0.451.. \n",
      "Epoch 77/80.. Train loss: 0.326.. Test loss: 0.641.. Train IOU: 0.546.. Test IOU: 0.481.. \n",
      "Epoch 78/80.. Train loss: 0.311.. Test loss: 0.604.. Train IOU: 0.543.. Test IOU: 0.442.. \n",
      "Epoch 78/80.. Train loss: 0.371.. Test loss: 0.623.. Train IOU: 0.523.. Test IOU: 0.474.. \n",
      "Epoch 79/80.. Train loss: 0.334.. Test loss: 0.630.. Train IOU: 0.561.. Test IOU: 0.428.. \n",
      "Epoch 79/80.. Train loss: 0.315.. Test loss: 0.632.. Train IOU: 0.557.. Test IOU: 0.475.. \n",
      "Epoch 79/80.. Train loss: 0.343.. Test loss: 0.674.. Train IOU: 0.530.. Test IOU: 0.482.. \n",
      "Epoch 80/80.. Train loss: 0.324.. Test loss: 0.622.. Train IOU: 0.554.. Test IOU: 0.433.. \n",
      "Epoch 80/80.. Train loss: 0.322.. Test loss: 0.638.. Train IOU: 0.551.. Test IOU: 0.469.. \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print('Loading data: \\n')\n",
    "train_fns, test_fns, df_masks, files_list = load_data()\n",
    "\n",
    "# Training presets\n",
    "batch_size = 16\n",
    "epochs = 80\n",
    "learning_rate = 0.00001\n",
    "test_split = .1\n",
    "\n",
    "original_size = 1024\n",
    "width = 224\n",
    "height = 224\n",
    "\n",
    "# Create dataset and data loader\n",
    "print('Preparing the dataset: \\n')\n",
    "train_ds = PneumothoraxDataset(train_fns, df_masks, files_list, transform=True, size = (height, width), mode = 'train')\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_ds)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "valid_ds = PneumothoraxDataset(test_fns, None, None, transform=False, size = (height, width), mode = 'validation')\n",
    "validloader = DataLoader(valid_ds, batch_size=8, shuffle=False, num_workers=1)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Prepare for training: initialize model, loss function, optimizer\n",
    "class param:\n",
    "    unet_depth = 6\n",
    "    unet_start_filters = 8\n",
    "model = UNet16(num_classes=1, pretrained = True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup device for training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "print('Train the model: \\n')\n",
    "\n",
    "train_stats_df = pd.DataFrame(columns = ['Epoch','Train loss','Test loss'])\n",
    "train(model, device, trainloader, testloader, optimizer, epochs)\n",
    "\n",
    "# Save the model\n",
    "#print('Save the model: \\n')\n",
    "#filepath = 'simple_unet.pth'\n",
    "#checkpoint = {'state_dict': model.state_dict()}\n",
    "#torch.save(checkpoint, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ddc5be3f054657b1d1c28c8520c602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=173), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission = {'ImageId': [], 'EncodedPixels': []}\n",
    "\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for X, fns in tqdm_notebook(validloader):\n",
    "    X = Variable(X).cuda()\n",
    "    output = model(X)\n",
    "    \n",
    "    for i, fname in enumerate(fns):\n",
    "        mask = torch.sigmoid(output[i].reshape(224,224)).data.cpu().numpy()\n",
    "        mask = binary_opening(mask > 0.9, disk(2))\n",
    "        \n",
    "        im = Image.fromarray(((mask)*255).astype(np.uint8)).resize((original_size,original_size))\n",
    "        im = np.transpose(np.asarray(im))\n",
    "        \n",
    "        submission['EncodedPixels'].append(mask2rle(im, original_size, original_size))\n",
    "        submission['ImageId'].append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6347.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6482.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.684.1517875164...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6336.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6736.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.631.1517875163...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5842.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6794.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6360.151787519...</td>\n",
       "      <td>411858 5 1019 5 1019 5 1019 5 1019 5 1015 13 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5959.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ImageId                                      EncodedPixels\n",
       "602   1.2.276.0.7230010.3.1.4.8323329.6347.151787519...                                                 -1\n",
       "749   1.2.276.0.7230010.3.1.4.8323329.6482.151787519...                                                 -1\n",
       "1142  1.2.276.0.7230010.3.1.4.8323329.684.1517875164...                                                 -1\n",
       "590   1.2.276.0.7230010.3.1.4.8323329.6336.151787519...                                                 -1\n",
       "1028  1.2.276.0.7230010.3.1.4.8323329.6736.151787519...                                                 -1\n",
       "561   1.2.276.0.7230010.3.1.4.8323329.631.1517875163...                                                 -1\n",
       "50    1.2.276.0.7230010.3.1.4.8323329.5842.151787519...                                                 -1\n",
       "1092  1.2.276.0.7230010.3.1.4.8323329.6794.151787520...                                                 -1\n",
       "617   1.2.276.0.7230010.3.1.4.8323329.6360.151787519...  411858 5 1019 5 1019 5 1019 5 1019 5 1015 13 1...\n",
       "177   1.2.276.0.7230010.3.1.4.8323329.5959.151787519...                                                 -1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(submission, columns=['ImageId', 'EncodedPixels'])\n",
    "submission_df.loc[submission_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\n",
    "submission_df.to_csv('submission09.csv', index=False)\n",
    "submission_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a3b2c7ab364b93ad00f8616b3f2d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=173), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for X, fns in tqdm_notebook(validloader):\n",
    "    X = Variable(X).cuda()\n",
    "    output = model(X)\n",
    "    \n",
    "    for i, fname in enumerate(fns):\n",
    "        mask = torch.sigmoid(output[i].reshape(224,224)).data.cpu().numpy()\n",
    "        mask = binary_opening(mask > 0.5, disk(2))\n",
    "        \n",
    "        im = Image.fromarray(((mask)*255).astype(np.uint8)).resize((original_size,original_size))\n",
    "        im = np.transpose(np.asarray(im))\n",
    "        \n",
    "        submission['EncodedPixels'].append(mask2rle(im, original_size, original_size))\n",
    "        submission['ImageId'].append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6807.151787520...</td>\n",
       "      <td>932658 5 32 4 983 5 32 4 983 5 32 4 983 5 32 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6198.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6940.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6767.151787520...</td>\n",
       "      <td>683182 4 1020 4 1020 4 1020 4 1020 4 1015 14 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6740.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6575.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6678.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.600.1517875163...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6659.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6749.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ImageId                                      EncodedPixels\n",
       "1106  1.2.276.0.7230010.3.1.4.8323329.6807.151787520...  932658 5 32 4 983 5 32 4 983 5 32 4 983 5 32 4...\n",
       "439   1.2.276.0.7230010.3.1.4.8323329.6198.151787519...                                                 -1\n",
       "1253  1.2.276.0.7230010.3.1.4.8323329.6940.151787520...                                                 -1\n",
       "1062  1.2.276.0.7230010.3.1.4.8323329.6767.151787520...  683182 4 1020 4 1020 4 1020 4 1020 4 1015 14 1...\n",
       "2410  1.2.276.0.7230010.3.1.4.8323329.6740.151787519...                                                 -1\n",
       "2228  1.2.276.0.7230010.3.1.4.8323329.6575.151787519...                                                 -1\n",
       "964   1.2.276.0.7230010.3.1.4.8323329.6678.151787519...                                                 -1\n",
       "222   1.2.276.0.7230010.3.1.4.8323329.600.1517875163...                                                 -1\n",
       "943   1.2.276.0.7230010.3.1.4.8323329.6659.151787519...                                                 -1\n",
       "2419  1.2.276.0.7230010.3.1.4.8323329.6749.151787519...                                                 -1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(submission, columns=['ImageId', 'EncodedPixels'])\n",
    "submission_df.loc[submission_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\n",
    "submission_df.to_csv('submission05.csv', index=False)\n",
    "submission_df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "18419fe2fbf14187b07c29f98e492b5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "22ddc5be3f054657b1d1c28c8520c602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f319b8097fe3449d9d80207ae79ef70b",
        "IPY_MODEL_a7ee6ae905094394a5318ae5e912186f"
       ],
       "layout": "IPY_MODEL_f825bd948c1b4a1d8ddbcfd40e071335"
      }
     },
     "3936f91863374c5da6268018e89ee205": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f2ab74b9073749a4bd24e83588bbc42b",
       "placeholder": "​",
       "style": "IPY_MODEL_e2c8995950ec4663ad91d39f82916ad6",
       "value": "100% 173/173 [10:51&lt;00:00,  2.78s/it]"
      }
     },
     "409d96bc01904f698644fa72a7ea8d6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4f43e89afb844c33aaca976960036add": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6109ec7ea22043a4b0d34c11df973d9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "67a3b2c7ab364b93ad00f8616b3f2d94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bb468994f2df4f16a4cb66763206cdf4",
        "IPY_MODEL_3936f91863374c5da6268018e89ee205"
       ],
       "layout": "IPY_MODEL_6e8197e3d7db438591f0c857412476ec"
      }
     },
     "6e8197e3d7db438591f0c857412476ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7ee6ae905094394a5318ae5e912186f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f43e89afb844c33aaca976960036add",
       "placeholder": "​",
       "style": "IPY_MODEL_409d96bc01904f698644fa72a7ea8d6e",
       "value": "100% 173/173 [10:25&lt;00:00,  2.78s/it]"
      }
     },
     "b9755ebc4918469ba60977e45c1f08be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb468994f2df4f16a4cb66763206cdf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b9755ebc4918469ba60977e45c1f08be",
       "max": 173,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_18419fe2fbf14187b07c29f98e492b5d",
       "value": 173
      }
     },
     "c8510b9cf56d4fec84f94bd58bcaff71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e2c8995950ec4663ad91d39f82916ad6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f2ab74b9073749a4bd24e83588bbc42b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f319b8097fe3449d9d80207ae79ef70b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c8510b9cf56d4fec84f94bd58bcaff71",
       "max": 173,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6109ec7ea22043a4b0d34c11df973d9f",
       "value": 173
      }
     },
     "f825bd948c1b4a1d8ddbcfd40e071335": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
