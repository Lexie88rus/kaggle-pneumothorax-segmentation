{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch UNet Model for Pneumothorax Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pydicom\n",
    "import random\n",
    "\n",
    "# import image manipulation\n",
    "from PIL import Image\n",
    "\n",
    "# import matplotlib for visualization\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "\n",
    "# Import rle utils\n",
    "import sys\n",
    "sys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n",
    "from mask_functions import rle2mask, mask2rle # import mask utilities\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Augmentor\r\n",
      "  Downloading https://files.pythonhosted.org/packages/49/d2/c7ab65de584652ebe19be8bce4de4a2c12d53ac6387a762c932552891433/Augmentor-0.2.5-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: future>=0.16.0 in /opt/conda/lib/python3.6/site-packages (from Augmentor) (0.17.1)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from Augmentor) (1.16.4)\r\n",
      "Requirement already satisfied: Pillow>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from Augmentor) (6.0.0)\r\n",
      "Requirement already satisfied: tqdm>=4.9.0 in /opt/conda/lib/python3.6/site-packages (from Augmentor) (4.32.1)\r\n",
      "Installing collected packages: Augmentor\r\n",
      "Successfully installed Augmentor-0.2.5\r\n"
     ]
    }
   ],
   "source": [
    "! pip install Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading utility\n",
    "def load_data(datafilepath = '../input/siim-train-test/siim/', healthy_num = 2000):\n",
    "    '''\n",
    "    Function to load the dataset.\n",
    "    INPUT:\n",
    "        datafilepath - path to directory containing the dataset.\n",
    "    OUTPUT:\n",
    "        train_fns - train dataset\n",
    "        train_fns - test dataset\n",
    "        df_masks - pandas dataframe containing masks for train dataset\n",
    "    '''\n",
    "    # Load full training and test sets\n",
    "    train_fns = sorted(glob(datafilepath + 'dicom-images-train/*/*/*.dcm'))\n",
    "    test_fns = sorted(glob(datafilepath + 'dicom-images-test/*/*/*.dcm'))\n",
    "    # Load csv masks\n",
    "    df_masks = pd.read_csv(datafilepath + 'train-rle.csv', index_col='ImageId')\n",
    "    # create a list of filenames with images to use\n",
    "    \n",
    "    counter = 0\n",
    "    files_list = []\n",
    "    for fname in train_fns:\n",
    "        try:\n",
    "            if '-1' in df_masks.loc[fname.split('/')[-1][:-4],' EncodedPixels']:\n",
    "                if counter <= 2000:\n",
    "                    files_list.append(fname)\n",
    "                    counter += 1\n",
    "            else:\n",
    "                files_list.append(fname)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return train_fns, test_fns, df_masks, files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    \"\"\"\n",
    "    Function performs the linear normalizarion of the array.\n",
    "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
    "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
    "    INPUT:\n",
    "        arr - orginal numpy array\n",
    "    OUTPUT:\n",
    "        arr - normalized numpy array\n",
    "    \"\"\"\n",
    "    arr = arr.astype('float')\n",
    "    # Do not touch the alpha channel\n",
    "    for i in range(1):\n",
    "        minval = arr[...,i].min()\n",
    "        maxval = arr[...,i].max()\n",
    "        if minval != maxval:\n",
    "            arr[...,i] -= minval\n",
    "            arr[...,i] *= (255.0/(maxval-minval))\n",
    "    return arr\n",
    "\n",
    "def normalize_image(img):\n",
    "    \"\"\"\n",
    "    Function performs the normalization of the image.\n",
    "    https://stackoverflow.com/questions/7422204/intensity-normalization-of-image-using-pythonpil-speed-issues\n",
    "    INPUT:\n",
    "        image - PIL image to be normalized\n",
    "    OUTPUT:\n",
    "        new_img - PIL image normalized\n",
    "    \"\"\"\n",
    "    arr = np.array(img)\n",
    "    new_img = Image.fromarray(normalize(arr).astype('uint8'),'L')\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class PneumothoraxDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for pneumothorax segmentation.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, fns, df_masks, files_list, transform=True, size = (224, 224), mode = 'train'):\n",
    "        '''\n",
    "        INPUT:\n",
    "            fns - glob containing the images\n",
    "            df_masks - dataframe containing image masks\n",
    "            transform (optional) - enable transforms for the images\n",
    "        '''\n",
    "        self.labels_frame = df_masks\n",
    "        self.fns = fns\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "        self.transforms = transforms.Compose([transforms.Resize(self.size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "        self.mode = mode\n",
    "        self.files_list = files_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if (self.mode == 'validation'):\n",
    "            return len(self.fns)\n",
    "        else:\n",
    "            return len(self.files_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Function to get items from dataset by idx.\n",
    "        INPUT:\n",
    "            idx - id of the image in the dataset\n",
    "        '''\n",
    "        # image height and width\n",
    "        im_height = 1024\n",
    "        im_width = 1024\n",
    "        # image channels\n",
    "        im_chan = 1\n",
    "\n",
    "        # get train image and mask\n",
    "        np_image = np.zeros((im_height, im_width, im_chan), dtype=np.uint8)\n",
    "        np_mask = np.zeros((im_height, im_width, 1), dtype=np.bool)\n",
    "        \n",
    "        # if validation then return filename instead of mask\n",
    "        if self.mode == 'validation':\n",
    "            # read dcm file with image\n",
    "            dataset = pydicom.read_file(self.fns[idx])\n",
    "            np_image = np.expand_dims(dataset.pixel_array, axis=2)\n",
    "        \n",
    "            image = Image.fromarray(np_image.reshape(im_height, im_width) , 'L')\n",
    "            image = self.transforms(image)\n",
    "            return [image, self.fns[idx].split('/')[-1][:-4]]\n",
    "        \n",
    "        # read dcm file with image\n",
    "        dataset = pydicom.read_file(self.files_list[idx])\n",
    "        np_image = np.expand_dims(dataset.pixel_array, axis=2)\n",
    "\n",
    "        # load mask\n",
    "        try:\n",
    "            # no pneumothorax\n",
    "            if '-1' in self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels']:\n",
    "                np_mask = np.zeros((im_height, im_width, 1), dtype=np.bool)\n",
    "            else:\n",
    "                # there is pneumothorax\n",
    "                if type(self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels']) == str:\n",
    "                    np_mask = np.expand_dims(rle2mask(self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels'], im_height, im_width), axis=2)\n",
    "                else:\n",
    "                    np_mask = np.zeros((1024, 1024, 1))\n",
    "                    for x in self.labels_frame.loc[self.files_list[idx].split('/')[-1][:-4],' EncodedPixels']:\n",
    "                        np_mask =  np_mask + np.expand_dims(rle2mask(x, 1024, 1024), axis=2)\n",
    "        except KeyError:\n",
    "            # couldn't find mask in dataframe\n",
    "            np_mask = np.zeros((im_height, im_width, 1), dtype=np.bool) # Assume missing masks are empty masks.\n",
    "\n",
    "        # convert to PIL\n",
    "        image = Image.fromarray(np_image.reshape(im_height, im_width) , 'L')\n",
    "        \n",
    "        np_mask = np.transpose(np_mask)\n",
    "        mask = Image.fromarray(np_mask.reshape(im_height, im_width).astype(np.uint8) , 'L')\n",
    "        \n",
    "        try:\n",
    "            # apply optional transforms (random rotation for the training set)\n",
    "            if self.transform:\n",
    "                # apply random rotation\n",
    "                angle = random.uniform(-5, 5)\n",
    "                #image = TF.rotate(image, angle)\n",
    "                #mask = TF.rotate(mask, angle)\n",
    "                \n",
    "                # replaced standard transformations with Augmentor rotate and zoom\n",
    "                p = Augmentor.DataPipeline([[np.array(image)], [np.array(mask)]])\n",
    "                p.rotate(1.0, max_left_rotation=5, max_right_rotation=5)\n",
    "                p.zoom_random(probability=0.3, percentage_area=0.95)\n",
    "                images_aug = p.sample(1)\n",
    "                image = images_aug[0][0]\n",
    "                mask = images_aug[0][1]\n",
    "                \n",
    "                # apply random horizontal flip\n",
    "                if (angle > 0):\n",
    "                    image = TF.hflip(image)\n",
    "                    mask = TF.hflip(mask)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # apply required transforms normalization, reshape and convert to tensor\n",
    "        image = normalize_image(image)\n",
    "        image = self.transforms(image)\n",
    "        mask = self.transforms(mask)\n",
    "        \n",
    "        # convert to tensor and clip mask\n",
    "        mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        mask = np.clip(mask, 0, 1)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/witwitchayakarn/u-net-with-pytorch\n",
    "\n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(in_channels,\n",
    "                     out_channels,\n",
    "                     kernel_size=1,\n",
    "                     groups=groups,\n",
    "                     stride=1)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True, groups=1):\n",
    "    return nn.Conv2d(in_channels,\n",
    "                     out_channels,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     bias=bias,\n",
    "                     groups=groups)\n",
    "\n",
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    if mode == 'transpose':\n",
    "        return nn.ConvTranspose2d(in_channels,\n",
    "                                  out_channels,\n",
    "                                  kernel_size=2,\n",
    "                                  stride=2)\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "            conv1x1(in_channels, out_channels))\n",
    "    \n",
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 convolutions and 1 MaxPool.\n",
    "    A ReLU activation follows each convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super(DownConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        if self.pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        before_pool = x\n",
    "        if self.pooling:\n",
    "            x = self.pool(x)\n",
    "        return x, before_pool\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 convolutions and 1 UpConvolution.\n",
    "    A ReLU activation follows each convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 merge_mode='concat',\n",
    "                 up_mode='transpose'):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.merge_mode = merge_mode\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.upconv = upconv2x2(self.in_channels,\n",
    "                                self.out_channels,\n",
    "                                mode=self.up_mode)\n",
    "\n",
    "        if self.merge_mode == 'concat':\n",
    "            self.conv1 = conv3x3(2*self.out_channels,\n",
    "                                 self.out_channels)\n",
    "        else:\n",
    "            # num of input channels to conv2 is same\n",
    "            self.conv1 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            from_down: tensor from the encoder pathway\n",
    "            from_up: upconv'd tensor from the decoder pathway\n",
    "        \"\"\"\n",
    "        from_up = self.upconv(from_up)\n",
    "        if self.merge_mode == 'concat':\n",
    "            x = torch.cat((from_up, from_down), 1)\n",
    "        else:\n",
    "            x = from_up + from_down\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    \"\"\" `UNet` class is based on https://arxiv.org/abs/1505.04597\n",
    "    The U-Net is a convolutional encoder-decoder neural network.\n",
    "    Contextual spatial information (from the decoding,\n",
    "    expansive pathway) about an input tensor is merged with\n",
    "    information representing the localization of details\n",
    "    (from the encoding, compressive pathway).\n",
    "    Modifications to the original paper:\n",
    "    (1) padding is used in 3x3 convolutions to prevent loss\n",
    "        of border pixels\n",
    "    (2) merging outputs does not require cropping due to (1)\n",
    "    (3) residual connections can be used by specifying\n",
    "        UNet(merge_mode='add')\n",
    "    (4) if non-parametric upsampling is used in the decoder\n",
    "        pathway (specified by upmode='upsample'), then an\n",
    "        additional 1x1 2d convolution occurs after upsampling\n",
    "        to reduce channel dimensionality by a factor of 2.\n",
    "        This channel halving happens with the convolution in\n",
    "        the tranpose convolution (specified by upmode='transpose')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, in_channels=1, depth=5,\n",
    "                 start_filts=64, up_mode='transpose',\n",
    "                 merge_mode='concat'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_channels: int, number of channels in the input tensor.\n",
    "                Default is 3 for RGB images.\n",
    "            depth: int, number of MaxPools in the U-Net.\n",
    "            start_filts: int, number of convolutional filters for the\n",
    "                first conv.\n",
    "            up_mode: string, type of upconvolution. Choices: 'transpose'\n",
    "                for transpose convolution or 'upsample' for nearest neighbour\n",
    "                upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        if up_mode in ('transpose', 'upsample'):\n",
    "            self.up_mode = up_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "                             \"upsampling. Only \\\"transpose\\\" and \"\n",
    "                             \"\\\"upsample\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        if merge_mode in ('concat', 'add'):\n",
    "            self.merge_mode = merge_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n",
    "                             \"merging up and down paths. \"\n",
    "                             \"Only \\\"concat\\\" and \"\n",
    "                             \"\\\"add\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        # NOTE: up_mode 'upsample' is incompatible with merge_mode 'add'\n",
    "        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n",
    "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n",
    "                             \"with merge_mode \\\"add\\\" at the moment \"\n",
    "                             \"because it doesn't make sense to use \"\n",
    "                             \"nearest neighbour to reduce \"\n",
    "                             \"depth channels (by half).\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.start_filts = start_filts\n",
    "        self.depth = depth\n",
    "\n",
    "        self.down_convs = []\n",
    "        self.up_convs = []\n",
    "\n",
    "        # create the encoder pathway and add to a list\n",
    "        for i in range(depth):\n",
    "            ins = self.in_channels if i == 0 else outs\n",
    "            outs = self.start_filts*(2**i)\n",
    "            pooling = True if i < depth-1 else False\n",
    "\n",
    "            down_conv = DownConv(ins, outs, pooling=pooling)\n",
    "            self.down_convs.append(down_conv)\n",
    "\n",
    "        # create the decoder pathway and add to a list\n",
    "        # - careful! decoding only requires depth-1 blocks\n",
    "        for i in range(depth-1):\n",
    "            ins = outs\n",
    "            outs = ins // 2\n",
    "            up_conv = UpConv(ins, outs, up_mode=up_mode,\n",
    "                merge_mode=merge_mode)\n",
    "            self.up_convs.append(up_conv)\n",
    "\n",
    "        self.conv_final = conv1x1(outs, self.num_classes)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_convs = nn.ModuleList(self.down_convs)\n",
    "        self.up_convs = nn.ModuleList(self.up_convs)\n",
    "\n",
    "        self.reset_params()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.xavier_normal(m.weight)\n",
    "            nn.init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "    def reset_params(self):\n",
    "        for i, m in enumerate(self.modules()):\n",
    "            self.weight_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_outs = []\n",
    "\n",
    "        # encoder pathway, save outputs for merging\n",
    "        for i, module in enumerate(self.down_convs):\n",
    "            x, before_pool = module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        for i, module in enumerate(self.up_convs):\n",
    "            before_pool = encoder_outs[-(i+2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        # No softmax is used. This means you need to use\n",
    "        # nn.CrossEntropyLoss is your training script,\n",
    "        # as this module includes a softmax already.\n",
    "        x = self.conv_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, trainloader, testloader, optimizer, criterion, epochs):\n",
    "    model.to(device)\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "\n",
    "            steps += 1\n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels.reshape(-1, 224, 224).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in testloader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model.forward(inputs)\n",
    "                        batch_loss = criterion(outputs, labels.reshape(-1, 224, 224).long())\n",
    "                        test_loss += batch_loss.item()\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                      f\"Test loss: {test_loss/len(testloader):.3f}.. \")\n",
    " \n",
    "                running_loss = 0\n",
    "                   \n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: \n",
      "\n",
      "Preparing the dataset: \n",
      "\n",
      "Train the model: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:198: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:199: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1.. Train loss: 0.139.. Test loss: 0.031.. \n",
      "Epoch 1/1.. Train loss: 0.031.. Test loss: 0.024.. \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print('Loading data: \\n')\n",
    "train_fns, test_fns, df_masks, files_list = load_data()\n",
    "\n",
    "# Training presets\n",
    "batch_size = 16\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "test_split = .1\n",
    "\n",
    "original_size = 1024\n",
    "width = 224\n",
    "height = 224\n",
    "\n",
    "# Create dataset and data loader\n",
    "print('Preparing the dataset: \\n')\n",
    "train_ds = PneumothoraxDataset(train_fns, df_masks, files_list, transform=True, size = (height, width), mode = 'train')\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_ds)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "valid_ds = PneumothoraxDataset(test_fns, None, None, transform=False, size = (height, width), mode = 'validation')\n",
    "validloader = DataLoader(valid_ds, batch_size=8, shuffle=False, num_workers=1)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Prepare for training: initialize model, loss function, optimizer\n",
    "class param:\n",
    "    unet_depth = 6\n",
    "    unet_start_filters = 8\n",
    "model = UNet(2, depth=param.unet_depth, start_filts=param.unet_start_filters, merge_mode='concat')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup device for training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "print('Train the model: \\n')\n",
    "\n",
    "train_stats_df = pd.DataFrame(columns = ['Epoch','Train loss','Test loss'])\n",
    "train(model, device, trainloader, testloader, optimizer, criterion, epochs)\n",
    "\n",
    "# Save the model\n",
    "#print('Save the model: \\n')\n",
    "#filepath = 'simple_unet.pth'\n",
    "#checkpoint = {'state_dict': model.state_dict()}\n",
    "#torch.save(checkpoint, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de71ee9a302b48cfb9b4f1691d7964eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=173), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission = {'ImageId': [], 'EncodedPixels': []}\n",
    "\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for X, fns in tqdm_notebook(validloader):\n",
    "    X = Variable(X).cuda()\n",
    "    output = model(X)\n",
    "    \n",
    "    X_flipped = torch.flip(X, dims = (3,))\n",
    "    output_flipped = torch.flip(model(X_flipped), dims = (3,))\n",
    "    \n",
    "    for i, fname in enumerate(fns):\n",
    "        mask = torch.sigmoid(output[i, 1]).data.cpu().numpy()\n",
    "        mask = binary_opening(mask > 0.5, disk(2))\n",
    "        \n",
    "        mask_flipped = torch.sigmoid(output_flipped[i, 1]).data.cpu().numpy()\n",
    "        mask_flipped = binary_opening(mask_flipped > 0.5, disk(2))\n",
    "        \n",
    "        im = Image.fromarray(((mask + mask_flipped) / 2 * 255).astype(np.uint8)).resize((original_size,original_size))\n",
    "        im = np.transpose(np.asarray(im))\n",
    "        \n",
    "        submission['EncodedPixels'].append(mask2rle(im, original_size, original_size))\n",
    "        submission['ImageId'].append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6347.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6482.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.684.1517875164...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6336.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6736.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.631.1517875163...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5842.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6794.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6360.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5959.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ImageId EncodedPixels\n",
       "602   1.2.276.0.7230010.3.1.4.8323329.6347.151787519...            -1\n",
       "749   1.2.276.0.7230010.3.1.4.8323329.6482.151787519...            -1\n",
       "1142  1.2.276.0.7230010.3.1.4.8323329.684.1517875164...            -1\n",
       "590   1.2.276.0.7230010.3.1.4.8323329.6336.151787519...            -1\n",
       "1028  1.2.276.0.7230010.3.1.4.8323329.6736.151787519...            -1\n",
       "561   1.2.276.0.7230010.3.1.4.8323329.631.1517875163...            -1\n",
       "50    1.2.276.0.7230010.3.1.4.8323329.5842.151787519...            -1\n",
       "1092  1.2.276.0.7230010.3.1.4.8323329.6794.151787520...            -1\n",
       "617   1.2.276.0.7230010.3.1.4.8323329.6360.151787519...            -1\n",
       "177   1.2.276.0.7230010.3.1.4.8323329.5959.151787519...            -1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(submission, columns=['ImageId', 'EncodedPixels'])\n",
    "submission_df.loc[submission_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6645.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6547.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6745.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6830.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6822.151787520...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6015.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6014.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.5980.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6372.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>1.2.276.0.7230010.3.1.4.8323329.6566.151787519...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ImageId EncodedPixels\n",
       "928   1.2.276.0.7230010.3.1.4.8323329.6645.151787519...            -1\n",
       "820   1.2.276.0.7230010.3.1.4.8323329.6547.151787519...            -1\n",
       "1038  1.2.276.0.7230010.3.1.4.8323329.6745.151787519...            -1\n",
       "1132  1.2.276.0.7230010.3.1.4.8323329.6830.151787520...            -1\n",
       "1123  1.2.276.0.7230010.3.1.4.8323329.6822.151787520...            -1\n",
       "239   1.2.276.0.7230010.3.1.4.8323329.6015.151787519...            -1\n",
       "238   1.2.276.0.7230010.3.1.4.8323329.6014.151787519...            -1\n",
       "201   1.2.276.0.7230010.3.1.4.8323329.5980.151787519...            -1\n",
       "630   1.2.276.0.7230010.3.1.4.8323329.6372.151787519...            -1\n",
       "841   1.2.276.0.7230010.3.1.4.8323329.6566.151787519...            -1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02f8616e83ac4218ba05750057760b8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42d9623fee9c474c800e3b8e54f97a8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e043d66c7484ac894a234884c47e6bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_02f8616e83ac4218ba05750057760b8a",
       "max": 173,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ecab936a3589434db4c57ef626e9397d",
       "value": 173
      }
     },
     "a05c0d272d8742a7875720a464859323": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c31dc2a33256468c88fd8d428488af9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a05c0d272d8742a7875720a464859323",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ddbd99ab28d245a5bf96063f99bba9a9",
       "value": "100% 173/173 [11:58&lt;00:00,  3.08s/it]"
      }
     },
     "ddbd99ab28d245a5bf96063f99bba9a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "de71ee9a302b48cfb9b4f1691d7964eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8e043d66c7484ac894a234884c47e6bc",
        "IPY_MODEL_c31dc2a33256468c88fd8d428488af9e"
       ],
       "layout": "IPY_MODEL_42d9623fee9c474c800e3b8e54f97a8c"
      }
     },
     "ecab936a3589434db4c57ef626e9397d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
